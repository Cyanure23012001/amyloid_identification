{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e468db7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdb_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Downloader\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdb_numpy\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdb_numpy'"
     ]
    }
   ],
   "source": [
    "# Downloader\n",
    "import pdb_numpy \n",
    "import requests\n",
    "import concurrent.futures\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "def download_pdb(pdb_id):\n",
    "    # Define the URL to download the PDB assembly file\n",
    "    local_filename = f\"{pdb_id}.pdb\"\n",
    "    if not os.path.exists(local_filename):\n",
    "        url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\n",
    "\n",
    "        # Use the requests module to get the content of the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Save the content of the response to the local filename\n",
    "        with open(local_filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "\n",
    "        \n",
    "    print(f\"Downloaded {local_filename}\")\n",
    "    #try:\n",
    "    pdb_line = PDB(local_filename)\n",
    "    #except:\n",
    "       # pdb_line = None\n",
    "        #pass\n",
    "    return pdb_line\n",
    "\n",
    "\n",
    "\n",
    "# Define the PDB IDs and assembly numbers to download\n",
    "pdb_ids = [\"6LU7\", \"7K93\", \"5L73\"]\n",
    "\"\"\"\n",
    "# Create a thread pool to download the PDB assemblies in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit each download task to the executor\n",
    "    futures = []\n",
    "    for pdb_id, assembly_num in zip(pdb_ids, assembly_nums):\n",
    "        futures.append(executor.submit(download_pdb_assembly, pdb_id, \"1\"))\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    concurrent.futures.wait(futures)\n",
    "\"\"\"\n",
    "download_pdb(\"7K93\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fc5b973",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pdb_numpy.coor.Coor at 0x7f49939fd570>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdb_numpy import Coor, Model\n",
    "from pdb_numpy.format import mmcif, pdb\n",
    "\n",
    "def cooredit(coor):\n",
    "    try:\n",
    "        local_rmsd = []\n",
    "        ref = coor.select_atoms(\"protein\")\n",
    "        ref_seq_dict = ref.get_aa_seq()\n",
    "\n",
    "        if len(ref_seq_dict) <= 6:\n",
    "            ref = ref.add_symmetry()\n",
    "            ref.compute_chains_CA()\n",
    "            ref_seq_dict = ref.get_aa_seq()\n",
    "            ref = ref.apply_transformation()\n",
    "            ref.compute_chains_CA()\n",
    "            ref_seq_dict = ref.get_aa_seq()\n",
    "            ref = ref.remove_overlap_chain()\n",
    "            ref_seq_dict = ref.get_aa_seq()\n",
    "\n",
    "        if len(ref_seq_dict) <= 6:\n",
    "            ref = ref.copy_box(x=2, y=2, z=2)\n",
    "            ref.compute_chains_CA()\n",
    "            ref = ref.remove_overlap_chain()\n",
    "            ref_seq_dict = ref.get_aa_seq()\n",
    "        return ref\n",
    "    except:\n",
    "        return coor\n",
    "\n",
    "coords = Coor('7K93.pdb')\n",
    "cooredit(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19538fd6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f1835",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%lprun -f PDB PDB(\"7K93.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3087e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "PDB(\"7K93.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b52ad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame()\n",
    "df4.iloc[0:0]\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('mehdi.pickle', 'rb') as f:\n",
    "    mehdi_list = pickle.load(f)\n",
    "    \n",
    "for struct in tqdm(mehdi_list):\n",
    "    #try:\n",
    "    new_row = download_pdb(struct) \n",
    "    amyloid = {'is_amyloid':True}\n",
    "    new_row2 = {**new_row,**amyloid}\n",
    "    df4 = df4.append(new_row2, ignore_index=True)\n",
    "    #except: \n",
    "     #   pass\n",
    "\n",
    "with open('TESTSET_500.pickle', 'rb') as f:\n",
    "    test_set = pickle.load(f)\n",
    "    \n",
    "for struct in tqdm(test_set):\n",
    "    try:\n",
    "        new_row = download_pdb(struct)\n",
    "        amyloid = {'is_amyloid':False}\n",
    "        new_row2 = {**new_row,**amyloid}\n",
    "        df4 = df4.append(new_row2, ignore_index=True)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9474f3c0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "76\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "df4 = pd.DataFrame()\n",
    "import copy\n",
    "# Suppréssion des données dans le df\n",
    "df4.iloc[0:0]\n",
    "\n",
    "py\n",
    "\n",
    "# Objectif: trouver les pdb dans test_set qui ne sont pas dans tempresults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586c21d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(tempresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbddaaa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import logging\n",
    "import time\n",
    "\n",
    "print(len(df4))\n",
    "time.sleep(5)\n",
    "logging.getLogger(\"pdb_numpy\").setLevel(logging.WARNING)\n",
    "start_time = time.time()\n",
    "def process_struct(struct, is_amyloid):\n",
    "    #try:\n",
    "    \n",
    "    new_row = download_pdb(struct)\n",
    "    if new_row != None:\n",
    "        amyloid = {'is_amyloid': is_amyloid}\n",
    "        new_row2 = {**new_row,**amyloid}\n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"Processing of {struct} completed in {processing_time} seconds\")\n",
    "        print(f\"{len(new_row2)}, PDB: {new_row2['PDB_ID']}\")\n",
    "        return new_row2\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "    #except:\n",
    "        #print (str(e))\n",
    "     #   return None\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "df4_list = []\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # Process the `mehdi_list` items\n",
    "    results = {executor.submit(process_struct, struct, True): struct for struct in mehdi_list}\n",
    "    for future in tqdm(concurrent.futures.as_completed(results), total=len(results)):\n",
    "        struct = results[future]\n",
    "        try:\n",
    "            new_row3 = future.result(timeout=60)\n",
    "            if new_row3 is not None:\n",
    "                df4_list.append(new_row3)\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            future.cancel()\n",
    "            print(f\"Processing of {struct} timed out after 60 seconds\")\n",
    "\n",
    "    # Process the `test_set` items\n",
    "    results = {executor.submit(process_struct, struct, False): struct for struct in test_set}\n",
    "    for future in tqdm(concurrent.futures.as_completed(results), total=len(results)):\n",
    "        struct = results[future]\n",
    "        try:\n",
    "            new_row4 = future.result(timeout=60)\n",
    "            if new_row4 is not None:\n",
    "                df4_list.append(new_row4)\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            future.cancel()\n",
    "            print(f\"Processing of {struct} timed out after 60 seconds\")\n",
    "    \n",
    "    print(len(df4))\n",
    "\n",
    "df4 = pd.DataFrame.from_dict(df4_list)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1a2294b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import dtale\n",
    "#dtale.show(df4)\n",
    "with open('tempresults','wb') as f:\n",
    "    pickle.dump(df4,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ccfa57cc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n"
     ]
    }
   ],
   "source": [
    "print(len(df4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44fde0e0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"475\"\n",
       "            src=\"http://rpbs-priv2:40000/dtale/iframe/2\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f49d82a8340>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 14:16:47,956 - INFO     - Executing shutdown due to inactivity...\n",
      "2023-05-09 14:16:47,963 - INFO     - Executing shutdown...\n",
      "2023-05-09 14:16:47,964 - ERROR    - Exception on /shutdown [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/meuret/anaconda3/lib/python3.10/site-packages/flask/app.py\", line 2525, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/home/meuret/anaconda3/lib/python3.10/site-packages/flask/app.py\", line 1822, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/home/meuret/anaconda3/lib/python3.10/site-packages/flask/app.py\", line 1820, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/home/meuret/anaconda3/lib/python3.10/site-packages/flask/app.py\", line 1796, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"/home/meuret/anaconda3/lib/python3.10/site-packages/dtale/app.py\", line 433, in shutdown\n",
      "    shutdown_server()\n",
      "  File \"/home/meuret/anaconda3/lib/python3.10/site-packages/dtale/app.py\", line 419, in shutdown_server\n",
      "    raise RuntimeError(\"Not running with the Werkzeug Server\")\n",
      "RuntimeError: Not running with the Werkzeug Server\n"
     ]
    }
   ],
   "source": [
    "dtale.show(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5eccaf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame()\n",
    "df5['Seq'] = df4['AAseq']\n",
    "print(df5)\n",
    "with open('DF5_with_AA.pickle','wb') as f:\n",
    "    pickle.dump(df5,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1002269",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7K93.pdb\n",
      "CANCELLED 7K93.pdb, 2244 AA\n"
     ]
    }
   ],
   "source": [
    "#### import pdb_numpy\n",
    "import pandas as pd\n",
    "import dtale \n",
    "from pdb_numpy.format import mmcif, pdb\n",
    "from pdb_numpy import Coor, Model\n",
    "import pdb_numpy.DSSP as DSSP\n",
    "import numpy as np\n",
    "import freesasa\n",
    "#from rdkit import Chem\n",
    "#from rdkit.Chem import GraphDescriptors\n",
    "#from rdkit.Chem import Descriptors\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import time\n",
    "def PDB(pdb_path):\n",
    "    print(pdb_path)\n",
    "    try:\n",
    "        coords = Coor(pdb_path)\n",
    "        test = coords.select_atoms(\"protein\")\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    sequenceAA = test.get_aa_seq()\n",
    "    temp = {val: key for key, val in sequenceAA.items()}\n",
    "    res = {val:key for key, val in temp.items()}\n",
    "\n",
    "    # Filter for structures too big\n",
    "    AAseq = ''.join(list(res.values())).replace('-','')\n",
    "    if len(AAseq) > 200:\n",
    "        print(f\"CANCELLED {pdb_path}, {len(AAseq)} AA\")\n",
    "        return None\n",
    "    #print(AAseq)\n",
    "    test = cooredit(test)\n",
    "    \n",
    "    #coor.write_pdb('7ZH7_new.pdb')\n",
    "    #print(test.transformation)\n",
    "    \n",
    "    # Calcul de la matrice de liaisons hydrogènes\n",
    "    try:\n",
    "        HBOND = DSSP.compute_Hbond_matrix(test).view()\n",
    "        result_dssp = DSSP.compute_DSSP(test)\n",
    "        #print(len(HBOND))\n",
    "\n",
    "        df = pd.DataFrame(HBOND)\n",
    "\n",
    "        # Calcul des noms de residus correspondants à chaque chaine\n",
    "        result = pd.unique([\"%d_%s\" % t for t in zip(test.resid, test.chain)])\n",
    "        #print(result)\n",
    "        #print(df)\n",
    "        df.columns = result\n",
    "        df.index = result\n",
    "\n",
    "        # Matrice Oui/non sur liaisons hydrogènes mais pour les résidus\n",
    "        coord = np.where(df)\n",
    "        coordinates = [(x,y) for x, y in zip(coord[0], coord[1])]\n",
    "        #print(coordinates)\n",
    "        names = pd.unique(test.chain)\n",
    "        df2 = pd.DataFrame(0, index=names, columns=names)\n",
    "\n",
    "        for coord in coordinates:\n",
    "            i, j = coord\n",
    "            #print(f\"Coords: {coord}, Index: {df.index[i]}, Column: {df.columns[j]}\")\n",
    "            row = df.index[i][-1:]\n",
    "            column = df.columns[j][-1:]\n",
    "            #print(f\"Row: {row}, Column = {column}\")\n",
    "            df2[column][row] += 1\n",
    "\n",
    "        #d = dtale.show(df,notebook=True)\n",
    "        #d = dtale.show(df2)\n",
    "        #print(d._main_url)\n",
    "\n",
    "        unique_combinations = []\n",
    "\n",
    "        # Determine le nombre de fois ou il y a des liaisons (matrice qui n'est pas symmetrique)\n",
    "        for i in names:\n",
    "            for j in names:\n",
    "                pair = [i,j]\n",
    "                if i != j and (pair[::-1] not in unique_combinations) :\n",
    "                    unique_combinations.append(pair)\n",
    "\n",
    "        #print(unique_combinations)\n",
    "        total = {}\n",
    "        numberofh = 0\n",
    "        for combinations in unique_combinations:\n",
    "            total[f\"{combinations[0]}_{combinations[1]}\"] = int(df2[combinations[0]][combinations[1]])+int(df2[combinations[1]][combinations[0]])\n",
    "            numberofh = numberofh + int(df2[combinations[0]][combinations[1]])+int(df2[combinations[1]][combinations[0]])\n",
    "\n",
    "        numbertotalh = numberofh/len(pd.unique(test.uniq_resid))*100\n",
    "        numberHchains = numberofh/(len(pd.unique(test.uniq_resid)) * len(pd.unique(test.chain)))\n",
    "    except:\n",
    "        return None\n",
    "    #Ecriture du fichier PDB\n",
    "    #pathpdb = f\"DOWN/{pdb_code}.pdb\"\n",
    "    #test.write(pathpdb)\n",
    "    \n",
    "    #Calcul avec FreeSasa du SASA\n",
    "    structure = freesasa.Structure(pdb_path)\n",
    "    result = freesasa.calc(structure)\n",
    "    print(result.totalArea())\n",
    "    area_classes = freesasa.classifyResults(result, structure)\n",
    "    #for key in area_classes:\n",
    "        #print(area_classes[key])\n",
    "    \n",
    "    # Calcul du molwt et du logp\n",
    "    #m = Chem.rdmolfiles.MolFromPDBFile(pathpdb)\n",
    "    #mollogppdb = Descriptors.MolLogP(m)\n",
    "    #molwtpdb = Descriptors.ExactMolWt(m)\n",
    "    \n",
    "    #print(result_dssp)\n",
    "    #print(type(result_dssp))\n",
    "    \n",
    "    # Calcul de la frequence de chaque lettre avec DSSP\n",
    "    df3=pd.DataFrame.from_dict(result_dssp[0],orient=\"index\", columns=[\"DSSP\"])\n",
    "    conc = \"\"\n",
    "    for item in result_dssp[0]:\n",
    "        conc += result_dssp[0][item]\n",
    "    conc = conc.replace(\" \", \"\")\n",
    "    #print(conc)\n",
    "    DSSP_letters = [\"G\",\"H\",\"I\",\"T\",\"E\",\"B\",\"S\"]\n",
    "    letter_freq = Counter(conc)\n",
    "\n",
    "    \n",
    "    for lettre in DSSP_letters:\n",
    "        if lettre not in letter_freq:\n",
    "            letter_freq[lettre] = 0\n",
    "        else:\n",
    "            letter_freq[lettre] = letter_freq[lettre]/len(conc)\n",
    "            \n",
    "    df4 = pd.DataFrame()\n",
    "    \n",
    "    #Alignement du DSSP\n",
    "    try:\n",
    "        total, alignement_total = alignement(result_dssp)\n",
    "        total_hydrophobicity,total_hydropathy = hydro_calculations(result)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    descriptors = {'PDB_ID':pdb_path[:4], 'Number of H':numbertotalh, 'H number w chains':numberHchains, 'SASA':result.totalArea(), 'Alignement_total':total, 'Alignement_moyen': alignement_total, 'DF_DSSP':df3,'DF_H':df2, 'AAseq':AAseq, 'Hydrophobicity':total_hydrophobicity, 'Hydropathy':total_hydropathy}\n",
    "    new_row = {**descriptors,**letter_freq}\n",
    "    \n",
    "    return new_row\n",
    "\n",
    "PDB('7K93.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d527cf0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from Bio import Align\n",
    "from Bio.Align import PairwiseAligner\n",
    "import pdb_numpy.DSSP as DSSP\n",
    "import pdb_numpy\n",
    "import dtale\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "aligner = PairwiseAligner()\n",
    "def alignement(result_dssp):\n",
    "    count = []\n",
    "    longueur = []\n",
    "    chains_letters = []\n",
    "\n",
    "    for chain in result_dssp[0]:\n",
    "        chains_letters.append(chain)\n",
    "        #print(result_dssp[0][chain])\n",
    "        # E beta ladder\n",
    "        # B beta bridge\n",
    "        # S Bend \n",
    "        count.append(result_dssp[0][chain].count('E'))\n",
    "        longueur.append(len(result_dssp[0][chain]))\n",
    "    #print(len(chains_letters))\n",
    "    \n",
    "    alignements_results = []\n",
    "    for value in range(1,len(chains_letters)):\n",
    "        alignement = aligner.align(result_dssp[0][chains_letters[0]],result_dssp[0][chains_letters[value]])\n",
    "        #print(alignement.score)\n",
    "        bit_score = (alignement.score / len(result_dssp[0][chains_letters[0]])*100)\n",
    "        #print(bit_score)\n",
    "        alignements_results.append(bit_score)\n",
    "                \n",
    "    countnp = np.array(count)\n",
    "    longueurnp = np.array(longueur)\n",
    "    resultats_alignements_np = np.array(alignements_results)\n",
    "\n",
    "\n",
    "    total = (np.sum(countnp)/np.sum(longueurnp))*100\n",
    "    alignement_moyen = np.mean(resultats_alignements_np) \n",
    "    #print(total)  \n",
    "    #print(alignement_moyen)\n",
    "\n",
    "    return total, alignement_moyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a156c49",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'freesasa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m     total_hydropathy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(dicthydropathy\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_hydrophobicity,total_hydropathy\n\u001b[0;32m---> 70\u001b[0m structure \u001b[38;5;241m=\u001b[39m \u001b[43mfreesasa\u001b[49m\u001b[38;5;241m.\u001b[39mStructure(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5K2G.pdb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     71\u001b[0m result \u001b[38;5;241m=\u001b[39m freesasa\u001b[38;5;241m.\u001b[39mcalc(structure)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(hydro_calculations(result))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'freesasa' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def hydro_calculations(freesasa_result):\n",
    "    # Eisenberg scale \n",
    "    # source: https://resources.qiagenbioinformatics.com/manuals/clcgenomicsworkbench/650/Hydrophobicity_scales.html\n",
    "    hydrophobicity_scale = {\n",
    "        'ALA': 0.62,\n",
    "        'CYS': 0.29,\n",
    "        'ASP': -0.9,\n",
    "        'GLU': -0.74,\n",
    "        'PHE': 1.19,\n",
    "        'GLY': 0.48,\n",
    "        'HIS': -0.40,\n",
    "        'ILE': 1.38,\n",
    "        'LYS': -1.50,\n",
    "        'LEU': 1.06,\n",
    "        'MET': 0.64,\n",
    "        'ASN': -0.78,\n",
    "        'PRO': 0.12,\n",
    "        'GLN': -0.85,\n",
    "        'ARG': -2.53,\n",
    "        'SER': -0.18,\n",
    "        'THR': -0.05,\n",
    "        'VAL': 1.08,\n",
    "        'TRP': 0.81,\n",
    "        'TYR': 0.26\n",
    "    }\n",
    "\n",
    "    #source: DOI:10.2174/1874464810902030171\n",
    "    hydropathy_scale = {\n",
    "        'ALA': 1.8,\n",
    "        'CYS': 2.5,\n",
    "        'ASP': -3.5,\n",
    "        'GLU': -3.5,\n",
    "        'PHE': 2.8,\n",
    "        'GLY': -0.4,\n",
    "        'HIS': -3.2,\n",
    "        'ILE': 4.5,\n",
    "        'LYS': -3.9,\n",
    "        'LEU': 3.8,\n",
    "        'MET': 1.9,\n",
    "        'ASN': -3.5,\n",
    "        'PRO': -1.6,\n",
    "        'GLN': -3.5,\n",
    "        'ARG': -4.5,\n",
    "        'SER': -0.8,\n",
    "        'THR': -0.7,\n",
    "        'VAL': 4.2,\n",
    "        'TRP': -0.9,\n",
    "        'TYR': -1.3   \n",
    "    }\n",
    "\n",
    "    # Calculate SASA for entire structure\n",
    "    structure_sasa = freesasa_result\n",
    "    test = structure_sasa.residueAreas()\n",
    "    dicthydrophobicity = {}\n",
    "    dicthydropathy = {}\n",
    "    for x in test:\n",
    "        for y in test[x]:\n",
    "            #print(test[x][y].residueType)\n",
    "            #print(hydrophobicity_scale[test[x][y].residueType])\n",
    "            # Calcul de l'hydrophobicité relative à chaque résidu, en fonction du SASA\n",
    "            # doi:10.1016/s0021-9673(03)00182-1. ISSN 0021-9673. PMID 12877193, source wikipedia hydrophobicity scales\n",
    "            #print(float(hydrophobicity_scale[test[x][y].residueType])*test[x][y].total)\n",
    "            dicthydrophobicity[f'{x}_{y}'] = float(hydrophobicity_scale[test[x][y].residueType])*test[x][y].total\n",
    "            dicthydropathy[f'{x}_{y}'] = hydropathy_scale[test[x][y].residueType]\n",
    "\n",
    "    total_hydrophobicity = sum(dicthydrophobicity.values())\n",
    "    total_hydropathy = sum(dicthydropathy.values())\n",
    "    return total_hydrophobicity,total_hydropathy\n",
    "\n",
    "structure = freesasa.Structure('5K2G.pdb')\n",
    "result = freesasa.calc(structure)\n",
    "print(hydro_calculations(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b079e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(df4['is_amyloid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9c0977",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "d = dtale.show(df4)\n",
    "print(d._main_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ab6a9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open('DF4_MEHDI_RANDOM.pickle','wb') as f:\n",
    "    pickle.dump(df4,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3aa3eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df5 = df4.drop(['DF_DSSP', 'DF_H', 'PDB_ID'], axis=1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df5.drop('is_amyloid', axis=1), df5['is_amyloid'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa460e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57075dcf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a586d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter=10000)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd728dd7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "print(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556be590",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b3817",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable for the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of the random forest classifier: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b602cbc2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "for i in range(3):\n",
    "    tree = rf.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,feature_names=X_train.columns,filled=True,max_depht=2,impurity=False,proportion=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    display(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8a8bc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(amypro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb7ef29",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open('amypro.pickle', 'rb') as f:\n",
    "    amypro = pickle.load(f)\n",
    "\n",
    "df6 = pd.DataFrame()\n",
    "for struct in amypro:\n",
    "    try:\n",
    "        new_row = PDB(struct) \n",
    "        df6 = df6.append(new_row, ignore_index=True)\n",
    "    except: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7aa5c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open('DF6_amypro_calculated.pickle','wb') as f:\n",
    "    pickle.dump(df6,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8523e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "d = dtale.show(df6)\n",
    "print(d._main_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb2706",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df7 = df6.drop(['DF_DSSP', 'DF_H', 'PDB_ID'], axis=1)\n",
    "y_pred = rf.predict(df7)\n",
    "print(type(y_pred))\n",
    "print(len(y_pred))\n",
    "print(len(df6))\n",
    "df8 = df6.drop(['DF_DSSP', 'DF_H'], axis=1)\n",
    "df8['is_amyloid'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a39bf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "d = dtale.show(df8)\n",
    "print(d._main_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb7799",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from Bio import Align\n",
    "from Bio.Align import PairwiseAligner\n",
    "def align():\n",
    "    count = []\n",
    "    longueur = []\n",
    "    chains_letters = []\n",
    "    print(result_dssp)\n",
    "\n",
    "    for chain in result_dssp[0]:\n",
    "        chains_letters.append(chain)\n",
    "        #print(result_dssp[0][chain])\n",
    "        # E beta ladder\n",
    "        # B beta bridge\n",
    "        # S Bend \n",
    "        count.append(result_dssp[0][chain].count('E'))\n",
    "        longueur.append(len(result_dssp[0][chain]))\n",
    "    #print(len(chains_letters))\n",
    "    \n",
    "    alignements_results = []\n",
    "    for value in range(1,len(chains_letters)):\n",
    "        alignement = aligner.align(result_dssp[0][chains_letters[0]],result_dssp[0][chains_letters[value]])\n",
    "        #print(alignement.score)\n",
    "        bit_score = (alignement.score / len(result_dssp[0][chains_letters[0]])*100)\n",
    "        #print(bit_score)\n",
    "        alignements_results.append(bit_score)\n",
    "                \n",
    "    countnp = np.array(count)\n",
    "    longueurnp = np.array(longueur)\n",
    "    resultats_alignements_np = np.array(alignements_results)\n",
    "\n",
    "\n",
    "    total = (np.sum(countnp)/np.sum(longueurnp))*100\n",
    "    alignement_moyen = np.mean(resultats_alignements_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40291ffa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ecb80c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import nglview\n",
    "view = nglview.show_file('7ZH7_new.pdb')\n",
    " ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfeb291",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(df2, annot=True, cmap='coolwarm', ax=ax)\n",
    "ax.set_xlabel('Acceptor Residue')\n",
    "ax.set_ylabel('Donor Residue')\n",
    "ax.set_title('H-Bond Contacts between Protein Chains')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beedfe6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "lines = inspect.getsource(PDB)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce7f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.pickle', 'rb') as f:\n",
    "    res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28644a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e91f75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
